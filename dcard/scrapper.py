import json
import time
import random
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.edge.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import undetected_chromedriver as uc

# ================= è¨­å®šå€ =================
# é—œéµå­—è¨­å®šï¼šæ¯å€‹é—œéµå­—å¯æŒ‡å®šæ™‚é–“ç¯„åœ
# æ ¼å¼: {"keyword": "é—œéµå­—", "start_date": "YYYY-MM-DD", "end_date": "YYYY-MM-DD"}
KEYWORDS_CONFIG = [
    # {"keyword": "ç‹å¤§é™¸", "start_date": "2025-02-18", "end_date": "2025-02-25"},
    # {"keyword": "å¤é”", "start_date": "2025-10-21", "end_date": "2025-10-28"},
    # {"keyword": "ä¿®æ°æ¥·", "start_date": "2025-10-21", "end_date": "2025-10-28"},
    # {"keyword": "é˜¿é”", "start_date": "2025-11-05", "end_date": "2025-11-12"},
    {"keyword": "é™³é›¶ä¹", "start_date": "2025-05-14", "end_date": "2025-05-21"},
    # {"keyword": "é™³æŸéœ–", "start_date": "2025-10-21", "end_date": "2025-10-28"},
    {"keyword": "æ›¸å‰", "start_date": "2025-10-21", "end_date": "2025-10-28"},
    # {"keyword": "å°æ°", "start_date": "2025-10-21", "end_date": "2025-10-28"}
]

MAX_SCROLL_TIMES = 50  # æ¯ç¯‡æ–‡ç« è¦åœ¨ç•™è¨€å€æ²å‹•å¹¾æ¬¡ (è¼‰å…¥æ›´å¤šç•™è¨€)
# =========================================

def setup_driver():
    options = Options()
    # æ”¹ç‚ºä½¿ç”¨ä¹¾æ·¨çš„ç€è¦½å™¨ç’°å¢ƒï¼Œä¸å†è®€å–æœ¬æ©Ÿ User Data
    # é€™æ¨£åŸ·è¡Œæ™‚ä¸éœ€è¦é—œé–‰æ‚¨å¹³å¸¸ä½¿ç”¨çš„ç€è¦½å™¨
    
    # é¿å…ä¸€äº›è‡ªå‹•åŒ–æª¢æ¸¬çš„ flag
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # driver = webdriver.Edge(options=options)
    driver = uc.Chrome()
    return driver


def parse_date_from_element(date_text=None, datetime_attr=None):
    """
    è§£ææ—¥æœŸï¼Œå„ªå…ˆä½¿ç”¨ <time> æ¨™ç±¤çš„ datetime å±¬æ€§
    datetime_attr: ISO 8601 æ ¼å¼ï¼Œä¾‹å¦‚ "2025-11-29T10:13:36.863Z"
    date_text: é¡¯ç¤ºæ–‡å­—ï¼Œä¾‹å¦‚ "2024å¹´1æœˆ15æ—¥", "1æœˆ15æ—¥", "2å°æ™‚å‰" ç­‰
    """
    try:
        # å„ªå…ˆä½¿ç”¨ datetime å±¬æ€§ (ç²¾ç¢ºçš„ ISO 8601 æ ¼å¼)
        if datetime_attr:
            # è™•ç† ISO 8601 æ ¼å¼: 2025-11-29T10:13:36.863Z
            # ç§»é™¤æ¯«ç§’å’Œ Z æ™‚å€æ¨™è¨˜ï¼Œè½‰æ›ç‚º datetime ç‰©ä»¶
            if 'T' in datetime_attr:
                # ç§»é™¤ 'Z' ä¸¦è™•ç†æ¯«ç§’
                datetime_str = datetime_attr.replace('Z', '').split('.')[0]
                return datetime.strptime(datetime_str, "%Y-%m-%dT%H:%M:%S")
        
        # å‚™ç”¨æ–¹æ¡ˆï¼šè§£æé¡¯ç¤ºæ–‡å­—
        if date_text:
            date_text = date_text.strip()
            
            # è™•ç†ç›¸å°æ™‚é–“ (ä»Šå¤©ã€æ˜¨å¤©ã€Xå°æ™‚å‰ç­‰) - é€™äº›éƒ½è¦–ç‚ºæœ€è¿‘çš„æ–‡ç« 
            if any(keyword in date_text for keyword in ["å°æ™‚å‰", "åˆ†é˜å‰", "å‰›å‰›", "ä»Šå¤©", "æ˜¨å¤©"]):
                return datetime.now()
            
            # è™•ç†å®Œæ•´æ—¥æœŸæ ¼å¼: "2024å¹´1æœˆ15æ—¥"
            if "å¹´" in date_text and "æœˆ" in date_text:
                date_text = date_text.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
                return datetime.strptime(date_text, "%Y-%m-%d")
            
            # è™•ç†åªæœ‰æœˆæ—¥çš„æ ¼å¼: "1æœˆ15æ—¥" (å‡è¨­ç‚ºä»Šå¹´)
            if "æœˆ" in date_text and "æ—¥" in date_text:
                current_year = datetime.now().year
                date_text = date_text.replace("æœˆ", "-").replace("æ—¥", "")
                return datetime.strptime(f"{current_year}-{date_text}", "%Y-%m-%d")
        
        # å¦‚æœç„¡æ³•è§£æï¼Œè¿”å› None
        return None
    except:
        return None


def is_date_in_range(article_date, start_date_str, end_date_str):
    """
    æª¢æŸ¥æ–‡ç« æ—¥æœŸæ˜¯å¦åœ¨æŒ‡å®šç¯„åœå…§
    """
    if article_date is None:
        return True  # ç„¡æ³•è§£ææ—¥æœŸæ™‚ï¼Œä¿ç•™è©²æ–‡ç« 
    
    try:
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
        return start_date <= article_date <= end_date
    except:
        return True  # æ—¥æœŸæ ¼å¼éŒ¯èª¤æ™‚ï¼Œä¿ç•™è©²æ–‡ç« 


def scrape_keyword(driver, keyword, start_date, end_date):
    """
    çˆ¬å–å–®ä¸€é—œéµå­—çš„æ‰€æœ‰ç¬¦åˆæ™‚é–“ç¯„åœçš„æ–‡ç« 
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ é–‹å§‹æœå°‹é—œéµå­—: {keyword}")
    print(f"ğŸ“… æ™‚é–“ç¯„åœ: {start_date} ~ {end_date}")
    print(f"{'='*60}\n")
    
    search_url = f"https://www.dcard.tw/search?query={keyword}&sort=latest"
    driver.get(search_url)
    
    # ç­‰å¾…æœå°‹çµæœè¼‰å…¥
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//a[contains(@href, "/f/")]'))
        )
    except:
        print(f"âš ï¸  æœå°‹ '{keyword}' ç„¡çµæœæˆ–è¼‰å…¥å¤±æ•—")
        return []
    
    time.sleep(2)

    # æ”¶é›†æ–‡ç« é€£çµèˆ‡æ—¥æœŸ
    article_data_list = []
    max_scroll_attempts = 100  # å¢åŠ æ²å‹•æ¬¡æ•¸ä»¥ç¢ºä¿æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆæ—¥æœŸçš„æ–‡ç« 
    scroll_count = 0
    no_new_links_count = 0
    out_of_range_count = 0  # é€£çºŒè¶…å‡ºç¯„åœçš„æ–‡ç« æ•¸
    
    start_date_obj = datetime.strptime(start_date, "%Y-%m-%d")
    end_date_obj = datetime.strptime(end_date, "%Y-%m-%d")
    
    collected_urls = set()
    
    while scroll_count < max_scroll_attempts:
        # æ”¶é›†ç•¶å‰é é¢ä¸Šçš„æ–‡ç« 
        # æ‰¾å°‹æ–‡ç« å¡ç‰‡ï¼Œé€šå¸¸åŒ…å«é€£çµå’Œæ—¥æœŸ
        article_cards = driver.find_elements(By.XPATH, '//a[contains(@href, "/f/") and contains(@href, "/p/")]/..')
        
        previous_count = len(article_data_list)
        
        for card in article_cards:
            try:
                # å–å¾—æ–‡ç« é€£çµ
                link_elem = card.find_element(By.XPATH, './/a[contains(@href, "/f/") and contains(@href, "/p/")]')
                href = link_elem.get_attribute('href')
                
                if not href or href in collected_urls:
                    continue
                
                # å˜—è©¦å–å¾—æ—¥æœŸ - å„ªå…ˆå¾ <time> æ¨™ç±¤ç²å– datetime å±¬æ€§
                date_text = None
                datetime_attr = None
                
                try:
                    # å„ªå…ˆå˜—è©¦æ‰¾å°‹ <time> æ¨™ç±¤ä¸¦å–å¾— datetime å±¬æ€§
                    time_elem = card.find_element(By.XPATH, './/time')
                    datetime_attr = time_elem.get_attribute('datetime')
                    date_text = time_elem.text  # åŒæ™‚ä¹Ÿå–å¾—é¡¯ç¤ºæ–‡å­—ä½œç‚ºå‚™ç”¨
                except:
                    # å¦‚æœæ‰¾ä¸åˆ° <time> æ¨™ç±¤ï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨
                    try:
                        date_elem = card.find_element(By.XPATH, './/span[contains(@class, "date")] | .//span[contains(text(), "æœˆ") or contains(text(), "å°æ™‚")]')
                        date_text = date_elem.text
                    except:
                        # å¦‚æœæ‰¾ä¸åˆ°æ˜ç¢ºçš„æ—¥æœŸå…ƒç´ ï¼Œå˜—è©¦å¾æ•´å€‹å¡ç‰‡æ–‡å­—ä¸­å°‹æ‰¾
                        card_text = card.text
                        # ç°¡å–®çš„æ—¥æœŸé—œéµå­—åŒ¹é…
                        for line in card_text.split('\n'):
                            if any(keyword in line for keyword in ["æœˆ", "å°æ™‚å‰", "åˆ†é˜å‰", "æ˜¨å¤©", "ä»Šå¤©"]):
                                date_text = line
                                break
                
                # è§£ææ—¥æœŸ (å„ªå…ˆä½¿ç”¨ datetime å±¬æ€§)
                article_date = parse_date_from_element(date_text=date_text, datetime_attr=datetime_attr)
                
                # æª¢æŸ¥æ—¥æœŸæ˜¯å¦åœ¨ç¯„åœå…§
                if article_date:
                    if article_date < start_date_obj:
                        # æ–‡ç« å¤ªèˆŠï¼Œå› ç‚ºæŒ‰æœ€æ–°æ’åºï¼Œå¾Œé¢çš„æ–‡ç« ä¹Ÿæœƒæ›´èˆŠ
                        out_of_range_count += 1
                        if out_of_range_count >= 10:
                            print(f"âš ï¸  å·²é€£çºŒé‡åˆ° 10 ç¯‡è¶…å‡ºæ™‚é–“ç¯„åœçš„æ–‡ç« ï¼Œåœæ­¢æœå°‹")
                            scroll_count = max_scroll_attempts  # å¼·åˆ¶çµæŸ
                            break
                        continue
                    elif article_date > end_date_obj:
                        # æ–‡ç« å¤ªæ–°ï¼Œç¹¼çºŒæ‰¾
                        continue
                    else:
                        # æ–‡ç« åœ¨ç¯„åœå…§
                        out_of_range_count = 0
                else:
                    # ç„¡æ³•è§£ææ—¥æœŸï¼Œä¿ç•™è©²æ–‡ç« 
                    pass
                
                collected_urls.add(href)
                article_data_list.append({
                    "url": href,
                    "date": date_text,
                    "datetime": datetime_attr,  # ä¿å­˜åŸå§‹ ISO 8601 æ™‚é–“
                    "parsed_date": article_date.strftime("%Y-%m-%d %H:%M:%S") if article_date else "æœªçŸ¥"
                })
                print(f"ğŸ“Š å·²æ”¶é›† {len(article_data_list)} ç¯‡æ–‡ç«  (æ—¥æœŸ: {article_date.strftime('%Y-%m-%d %H:%M:%S') if article_date else date_text if date_text else 'æœªçŸ¥'})")
                
            except Exception as e:
                continue
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å¢æ–‡ç« 
        if len(article_data_list) == previous_count:
            no_new_links_count += 1
            if no_new_links_count >= 100:
                print(f"âš ï¸  å·²é€£çºŒ100æ¬¡æ²å‹•ç„¡æ–°æ–‡ç« ï¼Œåœæ­¢æœå°‹")
                break
        else:
            no_new_links_count = 0
        
        # å¦‚æœå·²ç¶“é‡åˆ°å¤ªå¤šè¶…å‡ºç¯„åœçš„æ–‡ç« ï¼Œåœæ­¢
        if out_of_range_count >= 10:
            break
        
        # å¾€ä¸‹æ²å‹•ä»¥è¼‰å…¥æ›´å¤šæ–‡ç« 
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(1.5, 2.5))
        scroll_count += 1
    
    print(f"\nğŸ“‹ é—œéµå­— '{keyword}' æ‰¾åˆ° {len(article_data_list)} ç¯‡ç¬¦åˆæ™‚é–“ç¯„åœçš„æ–‡ç« \n")
    
    # çˆ¬å–æ¯ç¯‡æ–‡ç« çš„è©³ç´°å…§å®¹
    results = []
    for index, article_info in enumerate(article_data_list):
        url = article_info["url"]
        print(f"[{index+1}/{len(article_data_list)}] æ­£åœ¨çˆ¬å–: {url}")
        driver.get(url)
        time.sleep(random.uniform(2, 4))

        article_data = {
            "keyword": keyword,
            "url": url,
            "date": article_info["parsed_date"],
            "title": "N/A",
            "content": "N/A",
            "comments": []
        }

        try:
            # --- æŠ“å–æ¨™é¡Œ ---
            title_elem = driver.find_element(By.TAG_NAME, "h1")
            article_data["title"] = title_elem.text

            # --- æŠ“å–æ–‡ç« å…§å®¹ ---
            try:
                content_elem = driver.find_element(By.XPATH, '//div[contains(@class, "c04j7q-0")] | //article//div[contains(@class, "phqjxq-0")]')
                if not content_elem:
                    content_elem = driver.find_element(By.CSS_SELECTOR, "article div")
                article_data["content"] = content_elem.text
            except:
                try:
                    full_article = driver.find_element(By.TAG_NAME, "article").text
                    article_data["content"] = full_article
                except:
                    article_data["content"] = "ç„¡æ³•æå–å…§å®¹"

            # --- æŠ“å–ç•™è¨€ ---
            print("   â””â”€â”€ æ­£åœ¨è¼‰å…¥ç•™è¨€...")
            
            last_height = driver.execute_script("return document.body.scrollHeight")
            scroll_attempts = 0
            
            while scroll_attempts < MAX_SCROLL_TIMES:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(1.5, 2.5))
                
                new_height = driver.execute_script("return document.body.scrollHeight")
                
                if new_height == last_height:
                    print("   â””â”€â”€ å·²åˆ°é”é é¢åº•éƒ¨ï¼Œé–‹å§‹æŠ“å–ç•™è¨€")
                    break
                
                last_height = new_height
                scroll_attempts += 1
                print(f"   â””â”€â”€ æ²å‹•ä¸­... ({scroll_attempts}/{MAX_SCROLL_TIMES})")
            
            comment_blocks = driver.find_elements(By.XPATH, '//div[contains(@id, "comment-")]')
            
            for comment in comment_blocks:
                try:
                    text_div = comment.find_element(By.XPATH, './/div[@class="d_xa_34 d_xj_2v c1ehvwc9"]/span')
                    comment_content = text_div.text
                    
                    if comment_content == "":
                        continue
                    article_data["comments"].append(comment_content)
                except:
                    continue

            print(f"   â””â”€â”€ æˆåŠŸæŠ“å– {len(article_data['comments'])} å‰‡ç•™è¨€")

        except Exception as e:
            print(f"   âŒ çˆ¬å–æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        results.append(article_data)
    
    return results


def scrape_dcard():
    """
    ä¸»å‡½æ•¸ï¼šä¾åºçˆ¬å–æ‰€æœ‰é—œéµå­—
    """
    driver = setup_driver()
    all_results = []
    
    try:
        for index, config in enumerate(KEYWORDS_CONFIG):
            keyword = config["keyword"]
            start_date = config["start_date"]
            end_date = config["end_date"]
            
            print(f"\n{'#'*70}")
            print(f"# è™•ç†ç¬¬ {index+1}/{len(KEYWORDS_CONFIG)} å€‹é—œéµå­—")
            print(f"{'#'*70}")
            
            # çˆ¬å–è©²é—œéµå­—çš„æ–‡ç« 
            results = scrape_keyword(driver, keyword, start_date, end_date)
            all_results.extend(results)
            
            # æ¯å€‹é—œéµå­—ä¹‹é–“ä¼‘æ¯ä¸€ä¸‹
            if index < len(KEYWORDS_CONFIG) - 1:
                print(f"\nâ¸ï¸  ä¼‘æ¯ 3 ç§’å¾Œç¹¼çºŒä¸‹ä¸€å€‹é—œéµå­—...\n")
                time.sleep(3)
        
        # å„²å­˜æ‰€æœ‰çµæœ
        output_file = f'dcard_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=4)
        
        print(f"\n{'='*70}")
        print(f"âœ… æ‰€æœ‰çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š ç¸½å…±çˆ¬å– {len(all_results)} ç¯‡æ–‡ç« ")
        print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜ç‚º {output_file}")
        print(f"{'='*70}\n")
        
        # è¼¸å‡ºå„é—œéµå­—çµ±è¨ˆ
        keyword_stats = {}
        for article in all_results:
            kw = article["keyword"]
            keyword_stats[kw] = keyword_stats.get(kw, 0) + 1
        
        print("\nğŸ“ˆ å„é—œéµå­—çˆ¬å–çµ±è¨ˆï¼š")
        for kw, count in keyword_stats.items():
            print(f"   - {kw}: {count} ç¯‡æ–‡ç« ")

    except Exception as e:
        print(f"ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        driver.quit()


if __name__ == "__main__":
    scrape_dcard()
