import time, csv, re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# ======================================
# 關鍵字（貼文 or 留言，有出現就收）
# ======================================
KEYWORDS = [
    "逃兵", "明星逃兵", "明星閃兵",
    "王大陸", "坤達", "修杰楷", "阿達",
    "陳零九", "陳柏霖", "書偉", "小杰",
    "energy", "Energy", "ENERGY"
]

# ======================================
# 啟動 selenium
# ======================================
chrome_options = Options()
chrome_options.add_experimental_option("detach", True)  # 不讓 Chrome 自動關閉
chrome_options.add_argument("--disable-blink-features=AutomationControlled")

driver = webdriver.Chrome(
    service=Service(ChromeDriverManager().install()),
    options=chrome_options
)

# ======================================
# Step 1：搜尋 Threads
# ======================================
SEARCH_URL = "https://www.threads.net/search?q=逃兵"
driver.get(SEARCH_URL)
time.sleep(3)

print("\n=== 開始掃描貼文列表 ===")

# 自動捲動載入更多貼文
for _ in range(10):
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)

# 抓到所有貼文網址（含 media）
elements = driver.find_elements(By.TAG_NAME, "a")
post_urls = []

for e in elements:
    url = e.get_attribute("href")
    if url and "/post/" in url:
        post_urls.append(url)

post_urls = list(dict.fromkeys(post_urls))  # 去重複
print(f"找到 {len(post_urls)} 篇貼文 URL")

# ======================================
# Step 2：進入每篇貼文抓內容與留言
# ======================================
results = []

for idx, url in enumerate(post_urls, start=1):
    print(f"\n>>> ({idx}/{len(post_urls)}) 進入貼文：{url}")
    driver.get(url)
    time.sleep(3)

    # 連續捲動讓留言載入
    for _ in range(10):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)

    # 抓所有文字節點（貼文＋留言都在 span）
    spans = driver.find_elements(By.CSS_SELECTOR, "span")

    for sp in spans:
        text = sp.text.strip()
        if not text:
            continue

        # 有關鍵字就存
        if any(kw.lower() in text.lower() for kw in KEYWORDS):
            results.append({
                "url": url,
                "content": text
            })
            print(f"✔ 抓到：{text[:30]}...")

print(f"\n=== 全部完成！共抓到 {len(results)} 筆資料 ===")

# ======================================
# Step 3：輸出 CSV
# ======================================
with open("threads_逃兵分析.csv", "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["url", "content"])
    for row in results:
        writer.writerow([row["url"], row["content"]])

print("檔案已輸出：threads_逃兵分析.csv")
